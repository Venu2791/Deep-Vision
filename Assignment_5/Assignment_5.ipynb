{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venu2791/Deep-Vision/blob/master/Assignment_5/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpaWKBSClzwa",
        "colab_type": "text"
      },
      "source": [
        "Importing classes from keras for building CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDcEGoJmxkG",
        "colab_type": "text"
      },
      "source": [
        "Plotting a sample image from train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "18af3099-37c5-440a-ecf8-71effe9d573f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5d1d980a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "9c333ae0-6201-477e-eb5e-5516aa0f8ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj_eC-PGnHEu",
        "colab_type": "text"
      },
      "source": [
        "**Shape of channels in each layer**\n",
        "\n",
        "1.  28X28X1      | 3X3X1X16             -> 26X26X16 </br>\n",
        "2.  26X26X16     | 3X3X16X32            -> 24X24X32</br> \n",
        "3.  24X24X32     | 1X1X32X10            -> 24X24X10   </br>\n",
        "4.  24X24X10   <---->maxpooling<---->   -> 12X12X10</br>\n",
        "5.  12X12X10     | 3X3X10X16            -> 10X10X16</br>\n",
        "6.  10X10X10     | 3X3X16X16            -> 8X8X16</br>\n",
        "7.  8X8X16       | 3X3X16X16            -> 6X6X16</br>\n",
        "8.  6X6X16       | 3X3X16X16            -> 4X4X16</br>\n",
        "9.  4X4X16       | 4X4X16X10            -> 1X1X10</br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "d78f0d3e-e699-48fc-979f-65d626eddb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) #22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_49 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,604\n",
            "Trainable params: 16,360\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UF-hRTHp6fR",
        "colab_type": "text"
      },
      "source": [
        "**Model.compile()**\n",
        "\n",
        "**Loss Function:** A loss function is used to optimize the parameter values in a neural network model. Loss functions map a set of parameter values for the network onto a scalar value that indicates how well those parameter accomplish the task the network is intended to do.\n",
        "\n",
        "**Optimizer:** This step selects an optimizer algorithm that manipulates the learning rate per time-step so as to update the weights in a manner that does not lead to a swinging behaviour around the minimum\n",
        "\n",
        "**Learning Rate - Step Decay :**\n",
        "High learning rate will lead to random to and fro moment of the vector around local minima while a slow learning rate results in getting stuck into false minima. Thus, knowing when to decay the learning rate can be hard to find out.We base our experiment on the principle of step decay. Here, we reduce the learning rate by a constant factor every  epochs. \n",
        "\n",
        "**Data Augumentation:**\n",
        "Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.\n",
        "Transforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more. This is considered to be a type of regularization, as we create variations in the training data it provides the generalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "4404148e-a8b0-417e-c7d7-d9854e7e67f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator()\n",
        "output= model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64),validation_data=(X_test,Y_test),steps_per_epoch=200, epochs=40,callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "#ouput=model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "200/200 [==============================] - 8s 38ms/step - loss: 0.8677 - acc: 0.7592 - val_loss: 0.2057 - val_acc: 0.9604\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.4453 - acc: 0.8720 - val_loss: 0.1388 - val_acc: 0.9728\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.3640 - acc: 0.8890 - val_loss: 0.1075 - val_acc: 0.9825\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.3218 - acc: 0.9003 - val_loss: 0.0919 - val_acc: 0.9834\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2953 - acc: 0.9077 - val_loss: 0.0813 - val_acc: 0.9863\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.2726 - acc: 0.9162 - val_loss: 0.0649 - val_acc: 0.9889\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2770 - acc: 0.9141 - val_loss: 0.0614 - val_acc: 0.9889\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.2505 - acc: 0.9227 - val_loss: 0.0580 - val_acc: 0.9905\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2444 - acc: 0.9259 - val_loss: 0.0518 - val_acc: 0.9901\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2229 - acc: 0.9331 - val_loss: 0.0479 - val_acc: 0.9909\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2160 - acc: 0.9347 - val_loss: 0.0453 - val_acc: 0.9906\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.2116 - acc: 0.9373 - val_loss: 0.0522 - val_acc: 0.9895\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2055 - acc: 0.9378 - val_loss: 0.0421 - val_acc: 0.9915\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2094 - acc: 0.9387 - val_loss: 0.0442 - val_acc: 0.9902\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.2008 - acc: 0.9402 - val_loss: 0.0398 - val_acc: 0.9920\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1989 - acc: 0.9395 - val_loss: 0.0404 - val_acc: 0.9912\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1841 - acc: 0.9430 - val_loss: 0.0360 - val_acc: 0.9925\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1913 - acc: 0.9406 - val_loss: 0.0345 - val_acc: 0.9917\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1848 - acc: 0.9434 - val_loss: 0.0360 - val_acc: 0.9924\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1769 - acc: 0.9449 - val_loss: 0.0337 - val_acc: 0.9921\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1755 - acc: 0.9436 - val_loss: 0.0331 - val_acc: 0.9923\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1772 - acc: 0.9452 - val_loss: 0.0335 - val_acc: 0.9928\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1824 - acc: 0.9430 - val_loss: 0.0332 - val_acc: 0.9915\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1690 - acc: 0.9462 - val_loss: 0.0310 - val_acc: 0.9929\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1703 - acc: 0.9473 - val_loss: 0.0318 - val_acc: 0.9936\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1657 - acc: 0.9463 - val_loss: 0.0316 - val_acc: 0.9927\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1729 - acc: 0.9436 - val_loss: 0.0288 - val_acc: 0.9926\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1598 - acc: 0.9495 - val_loss: 0.0310 - val_acc: 0.9923\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1596 - acc: 0.9501 - val_loss: 0.0270 - val_acc: 0.9934\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1554 - acc: 0.9487 - val_loss: 0.0279 - val_acc: 0.9924\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1534 - acc: 0.9502 - val_loss: 0.0289 - val_acc: 0.9930\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1609 - acc: 0.9467 - val_loss: 0.0291 - val_acc: 0.9927\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1604 - acc: 0.9507 - val_loss: 0.0280 - val_acc: 0.9935\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1495 - acc: 0.9519 - val_loss: 0.0273 - val_acc: 0.9935\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1594 - acc: 0.9455 - val_loss: 0.0266 - val_acc: 0.9936\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1570 - acc: 0.9477 - val_loss: 0.0282 - val_acc: 0.9929\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "200/200 [==============================] - 3s 16ms/step - loss: 0.1582 - acc: 0.9486 - val_loss: 0.0269 - val_acc: 0.9933\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1535 - acc: 0.9502 - val_loss: 0.0259 - val_acc: 0.9935\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1463 - acc: 0.9512 - val_loss: 0.0263 - val_acc: 0.9928\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.1537 - acc: 0.9464 - val_loss: 0.0259 - val_acc: 0.9933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "outputId": "df75b305-ec38-441c-ce54-77cde7caa839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025855406473763286, 0.9933]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJAg9-qyKVI",
        "colab_type": "text"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "Change Code from Assignment 4 to include:\n",
        "\n",
        "*   Image normalization\n",
        "*   L2 regularization\n",
        "*   ReLU after BN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2QAJpgNZ5Ef",
        "colab_type": "code",
        "outputId": "edbb989f-3597-4e30-9c88-6f207e274ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(16, 3, 3, input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) #22\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_57 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,604\n",
            "Trainable params: 16,360\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6IwmQPqzbB6",
        "colab_type": "text"
      },
      "source": [
        "***Input Image Normalization:***\n",
        "\n",
        "Standardize and Normalize the image using the featurewise_center (standardization) and featurewise_std_normalization (Normalization using mean and variance).\n",
        "Standardization is different from Normalization, the former refers to scaling the pixels(diving it by 255).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qDl21ozBnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLpk53eM1sZQ",
        "colab_type": "text"
      },
      "source": [
        "***L2 Regularization:***\n",
        "\n",
        "Regularization is a key component in preventing overfitting. Also, some techniques of regularization can be used to reduce model parameters while maintaining accuracy, for example, to drive some of the parameters to zero. This might be desirable for reducing the model size or driving down the cost of evaluation in a mobile environment where processor power is constrained.\n",
        "\n",
        "In the below step we have introduced the custom loss function for L2 Implementation. Here, we have added the loss to the total model rather than applying ffor each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjnM82TsMp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def custom_loss(lambd, model, batch_size):\n",
        "  \n",
        "  def loss(y_true, y_pred):\n",
        "    reg_loss = 0.0;\n",
        "    for layer in model.layers:\n",
        "      if \"Conv\" in str(layer):\n",
        "        reg_loss += K.sum(K.square(layer.weights[0]))\n",
        "    l2_loss = (lambd/(2*batch_size))*reg_loss\n",
        "    return K.categorical_crossentropy(y_true,y_pred) + l2_loss\n",
        "  return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFL6nnm03NHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "model.compile(loss=custom_loss(0.1, model, 512), optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "bestval = ModelCheckpoint('bestmodel.hdf5', save_best_only=True, monitor='val_acc', mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT_a0kLR3N66",
        "colab_type": "code",
        "outputId": "0a2927a5-36ad-4477-a3fd-2849825480fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "outputl2 = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64),validation_data=(X_test,Y_test),steps_per_epoch=200, epochs=40,callbacks=[bestval,LearningRateScheduler(scheduler, verbose=1)])\n",
        "#model.fit(X_train, Y_train,  batch_size=512, validation_data=(X_test,Y_test),epochs=15, verbose=1,callbacks=[lr_rate])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "200/200 [==============================] - 9s 44ms/step - loss: 1.0024 - acc: 0.7334 - val_loss: 0.4206 - val_acc: 0.9141\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.5297 - acc: 0.8609 - val_loss: 0.2216 - val_acc: 0.9723\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.4647 - acc: 0.8657 - val_loss: 0.1849 - val_acc: 0.9679\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.4247 - acc: 0.8722 - val_loss: 0.1645 - val_acc: 0.9779\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.4010 - acc: 0.8751 - val_loss: 0.1032 - val_acc: 0.9851\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3793 - acc: 0.8812 - val_loss: 0.1014 - val_acc: 0.9862\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3643 - acc: 0.8846 - val_loss: 0.0916 - val_acc: 0.9865\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3644 - acc: 0.8811 - val_loss: 0.0821 - val_acc: 0.9884\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3557 - acc: 0.8830 - val_loss: 0.0873 - val_acc: 0.9862\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3576 - acc: 0.8818 - val_loss: 0.0757 - val_acc: 0.9899\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3610 - acc: 0.8804 - val_loss: 0.0703 - val_acc: 0.9896\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3430 - acc: 0.8851 - val_loss: 0.0696 - val_acc: 0.9898\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3463 - acc: 0.8851 - val_loss: 0.0694 - val_acc: 0.9889\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3489 - acc: 0.8827 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3249 - acc: 0.8902 - val_loss: 0.0619 - val_acc: 0.9904\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3372 - acc: 0.8864 - val_loss: 0.0658 - val_acc: 0.9898\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3196 - acc: 0.8921 - val_loss: 0.0591 - val_acc: 0.9914\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3367 - acc: 0.8860 - val_loss: 0.0557 - val_acc: 0.9919\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3212 - acc: 0.8895 - val_loss: 0.0565 - val_acc: 0.9917\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3271 - acc: 0.8890 - val_loss: 0.0571 - val_acc: 0.9904\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3198 - acc: 0.8891 - val_loss: 0.0552 - val_acc: 0.9914\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3345 - acc: 0.8846 - val_loss: 0.0562 - val_acc: 0.9911\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3242 - acc: 0.8886 - val_loss: 0.0530 - val_acc: 0.9915\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3259 - acc: 0.8865 - val_loss: 0.0536 - val_acc: 0.9924\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3258 - acc: 0.8871 - val_loss: 0.0510 - val_acc: 0.9921\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3140 - acc: 0.8892 - val_loss: 0.0520 - val_acc: 0.9921\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3228 - acc: 0.8890 - val_loss: 0.0502 - val_acc: 0.9928\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3337 - acc: 0.8826 - val_loss: 0.0532 - val_acc: 0.9914\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3345 - acc: 0.8851 - val_loss: 0.0512 - val_acc: 0.9917\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3215 - acc: 0.8887 - val_loss: 0.0493 - val_acc: 0.9925\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3216 - acc: 0.8881 - val_loss: 0.0489 - val_acc: 0.9929\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3140 - acc: 0.8888 - val_loss: 0.0498 - val_acc: 0.9924\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3221 - acc: 0.8884 - val_loss: 0.0470 - val_acc: 0.9930\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3192 - acc: 0.8888 - val_loss: 0.0476 - val_acc: 0.9933\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3109 - acc: 0.8913 - val_loss: 0.0477 - val_acc: 0.9930\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3091 - acc: 0.8927 - val_loss: 0.0469 - val_acc: 0.9925\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.3155 - acc: 0.8896 - val_loss: 0.0475 - val_acc: 0.9926\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3079 - acc: 0.8935 - val_loss: 0.0457 - val_acc: 0.9933\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3103 - acc: 0.8916 - val_loss: 0.0442 - val_acc: 0.9933\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.3106 - acc: 0.8910 - val_loss: 0.0470 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MczPqhTtZLYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction=model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITnDjNVV1GfW",
        "colab_type": "text"
      },
      "source": [
        "***Misclassified Images:***\n",
        "\n",
        "We have taken top 25 misclassified images and made an image gallery out of it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7LTQ-RmTCRt",
        "colab_type": "code",
        "outputId": "dd11261c-9196-4681-fb05-450fa92b8166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "source": [
        "misclassified_idx = []\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if np.argmax(Y_test[i])!= np.argmax(prediction[i]):\n",
        "    misclassified_idx.append(i)\n",
        "\n",
        "plt_idx = np.random.choice(misclassified_idx, 25)\n",
        "fig, ax = plt.subplots(5,5,figsize = (10,10))\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(left=None, bottom=None, right=None, top=0.92, wspace=None, hspace=None)\n",
        "k = 0\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    img = X_test[plt_idx[k]].reshape(28,28)\n",
        "    pred = np.argmax(prediction[plt_idx[k]])\n",
        "    actual = np.argmax(Y_test[plt_idx[k]])\n",
        "    ax[i,j].set_xticklabels([])\n",
        "    ax[i,j].set_yticklabels([])\n",
        "    ax[i,j].imshow(img)\n",
        "    ax[i,j].set_title(\"Predicted: \" + format(pred) + \", Actual: \" + format(actual), fontsize=10)\n",
        "    k += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArQAAAKRCAYAAAClXzpfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzv0LPoh2s1U",
        "colab_type": "text"
      },
      "source": [
        "***Summary:***\n",
        "\n",
        "So far we have learnt multiple regularization techniques : Batch norm, L2 Regularization, Data augumentation, Learning Rate, Drop-Out. \n",
        "\n",
        "*Note:* When using multiple regularization techiniques it will have cummulative effect in the model. So it is important to use these iteratively and assess the performance at each step.\n"
      ]
    }
  ]
}