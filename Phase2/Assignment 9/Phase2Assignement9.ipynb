{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase2Assignement9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMX37gIpMqS7R+qm5RmQY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venu2791/Deep-Vision/blob/master/Phase2/Assignment%209/Phase2Assignement9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI-Bam_fw4wR",
        "colab_type": "code",
        "outputId": "75eab343-d41e-4fd6-88a2-ad58c104cc66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!pip install gym\n",
        "!pip install pybullet\n",
        "!pip install agents#!pip install ruamel.yaml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/07/e15c6084e4579e3af925c3845d7cd4442bca6f7ac499ce330c441451b004/pybullet-2.7.8-cp36-cp36m-manylinux1_x86_64.whl (95.2MB)\n",
            "\u001b[K     |████████████████████████████████| 95.2MB 51kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.8\n",
            "\u001b[31mERROR: Invalid requirement: 'agents#!pip'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ofHn6FamOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g8gdXmTt6tV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step 1 : Creating a Replay buffer with a memory size of 10e6, We the populate the new transition using the FIFO policy. Below for loop is used to update the batch with the experience replays.\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  def _init_(self,max_size=10e6):\n",
        "    self.storage=[]\n",
        "    self.max_size=max_size\n",
        "    self.ptr=0\n",
        "  def add(self,transition):\n",
        "    if len(self.storage)==self.max_size:\n",
        "      self.storage[int(self.ptr)]=transition\n",
        "      self.ptr=(self.ptr+1)%self.max_size\n",
        "    else:\n",
        "        self.storage.append(transition)\n",
        "  def sample(self, batch_size):\n",
        "    ind=np.random.randint(0,len(self.storage),batch_size)\n",
        "    batch_states,batch_next_states,batch_actions,batch_rewards,batch_dones=[],[],[],[],[],[]\n",
        "    for i in ind:\n",
        "      state,next_state,action,reward,done=self.storage[i]\n",
        "      batch_states.append(np.array(state,copy=False))\n",
        "      batch_next_states.append(np.array(next_state,copy=False))\n",
        "      batch_actions.append(np.array(action,copy=False))\n",
        "      batch_rewards.append(np.array(reward,copy=False))\n",
        "      batch_dones.append(np.array(done,copy=False))\n",
        "    return np.array(batch_States),np.array(batch_next_states),np.array(batch_actions),np.array(batch_rewards).reshape(-1,1),np.array(batch_dones).reshape(-1,1)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-uikbIB2RPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 2: Defining the Actor model (one for Model and one for Target)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "   def __init__(self, state_dims, action_dims, max_action):\n",
        "        # max action is to clip in case we added too much noise\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(state_dims, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "   def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "        return x\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXHWbbS3w2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step 3 : Defining the Critic Model (Two for Model and Two for Target). Both the critic models are given the forward pass at the same time.\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def _init_(self,state_dims,action_dim):\n",
        "    super(Critic,self)._init_()\n",
        "    #First Critic Network\n",
        "    self.layer_1=nn.Linear(state_dims_action_dim,400)\n",
        "    self.layer_2=nn.Linear(400,300)\n",
        "    self.layer_3=nn.Linear(300,action_dim)\n",
        "    #Second Critic Network\n",
        "    self.layer_4=nn.Linear(state_dims_action_dim,400)\n",
        "    self.layer_5=nn.Linear(400,300)\n",
        "    self.layer_6=nn.Linear(300,action_dim)\n",
        "  def forward(self,x,u):\n",
        "    xu=torch.cat([x,u],1)\n",
        "    x1=F.relu(self.layer_1(xu))\n",
        "    x1=F.relu(self.layer_2(x1))\n",
        "    x1=Self.layer_3(x1)\n",
        "\n",
        "    x2=F.relu(self.layer_4(xu))\n",
        "    x2=F.relu(self.layer_5(x2))\n",
        "    x2=Self.layer_6(x2)\n",
        "    return x1,x2\n",
        "\n",
        "  def Q1(self,x,u):\n",
        "    xu=torch.cat([x,u],1)\n",
        "    x1=F.relu(self.layer_1(xu))\n",
        "    x1=F.relu(self.layer_2(x1))\n",
        "    x1=self.layer_3(x1)\n",
        "    return x1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK-AYrgF-owq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ste4-15\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') # To use GPU if available\n",
        "\n",
        "class T3D (object):\n",
        "  def _init_(self,state_dims,action_dim,max_action):\n",
        "    self.actor=Actor(state_dims,action_dim,max_action).to(device) #call Actor class (Model)\n",
        "    self.actor_target=Actor(state_dims,action_dim,max_action).to(device) # call Actor class (Target). Both have same architecture, thus we use the same class.\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict) # load_state_dict - Loads a model's parameter dictionary using a deserialized state_dict.Using this, we initialize the same weights for the Target and Model.\n",
        "    self.actor_optimizer=torch.optim.Adam(self.actor.parameters()) \n",
        "\n",
        "    self.critic=Critic(state_dims,action_dim).to(device)#call Critic class (Model)\n",
        "    self.critic_target=Critic(state_dims,action_dim).to(device) # call Actor class (Target). \n",
        "    self.critic_target.load_state_dict(self.actor.state_dict)# load_state_dict - Loads a model's parameter dictionary using a deserialized state_dict.Using this, we initialize the same weights for the Target and Model.\n",
        "\n",
        "    self.critic_optimizer=torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action=max_action \n",
        "\n",
        "\n",
        "  def select_action(self,state):\n",
        "    state=torch.Tensor(state.reshape(-1,1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self,replay_buffer,iterations,batch_size=100,discount=0.99,tau=.005,policy_noise=0.2,noise_clip=0.5,policy_freq=2):\n",
        "    for it in range(iterations):\n",
        "      batch_states,batch_next_states,batch_actions,batch_rewards,batch_dones=replay_buffer.sample(batch_size)\n",
        "      state=torch.Tensor(batch_states).to(device)\n",
        "      next_state=torch.Tensor(batch_next_states)\n",
        "      action=torch.Tensor(batch_actions)\n",
        "      reward=torch.Tensor(batch_rewards)\n",
        "      done=torch.Tensor(batch_dones)\n",
        "      next_action=self.actor_target.forward(next_state)\n",
        "      noise=torch.Tensor(batch_actions).data.normal_(0,policy_noise).to(device)\n",
        "      \n",
        "      noise=noise.clamp(-noise_clip,noise_clip)\n",
        "      next_action=(next_action + noise).clamp(-self.max.action,self.max_action)\n",
        "      target_Q1,target_Q2=self.critic_target.forward(next_state,next_action)\n",
        "      target_Q=torch.min(target_Q1,target_Q2)\n",
        "      target_Q=reward+((1-done)*discount*target_Q).detach()\n",
        "      current_Q1,current_Q2=self.critic.forward(state,action)\n",
        "      critic_loss=F.mse_loss(currentQ1,target_Q)+F.mse_loss(current_Q2,target_Q)\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      if it % policy_freq==0:\n",
        "        actor_loss=-(self.critic.Q1(state,self.actor(state)).mean())\n",
        "        self.actor_optimizer.grad_zero()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        # Polyak Averaging for Actor\n",
        "      for param,target_param in zip(self.actor.parameters(),self.actor_target.parameters()):\n",
        "        target_param.data.copy_(tau*param.data+(1-tau)*target_param.data)\n",
        "        # Polyak Averaging for Critic\n",
        "      for param,target_param in zip(self.critic.parameters(),self.critic_target.parameters()):\n",
        "        target_param.data.copy_(tau*param.data+(1-tau)*target_param.data)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}